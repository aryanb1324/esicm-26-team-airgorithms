{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b600a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL rows: 233791\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_occurrence_id</th>\n",
       "      <th>measure_time</th>\n",
       "      <th>person_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>visit_start_datetime</th>\n",
       "      <th>visit_end_datetime</th>\n",
       "      <th>visit_type_concept_name</th>\n",
       "      <th>admitted_from</th>\n",
       "      <th>discharged_to</th>\n",
       "      <th>...</th>\n",
       "      <th>glucose_median_missing</th>\n",
       "      <th>bun_median_missing</th>\n",
       "      <th>creatinine_median_missing</th>\n",
       "      <th>crp_median_missing</th>\n",
       "      <th>dp_dyn</th>\n",
       "      <th>peep_t</th>\n",
       "      <th>delta_peep</th>\n",
       "      <th>dp_dyn_next</th>\n",
       "      <th>map_next</th>\n",
       "      <th>is_terminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>6656</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1942</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-18 08:52:00+00:00</td>\n",
       "      <td>EHR encounter record</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.15</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6656</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1942</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-18 08:52:00+00:00</td>\n",
       "      <td>EHR encounter record</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.15</td>\n",
       "      <td>7.85</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.05</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6656</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1942</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-18 08:52:00+00:00</td>\n",
       "      <td>EHR encounter record</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.05</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32.05</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6656</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1942</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-18 08:52:00+00:00</td>\n",
       "      <td>EHR encounter record</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.05</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6656</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1942</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-18 08:52:00+00:00</td>\n",
       "      <td>EHR encounter record</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>No matching concept</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>34.95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   visit_occurrence_id  measure_time  person_id gender  year_of_birth  \\\n",
       "0                    1            -1       6656   MALE           1942   \n",
       "1                    1             0       6656   MALE           1942   \n",
       "2                    1             1       6656   MALE           1942   \n",
       "3                    1             2       6656   MALE           1942   \n",
       "4                    1             3       6656   MALE           1942   \n",
       "\n",
       "        visit_start_datetime         visit_end_datetime  \\\n",
       "0  2017-01-01 00:00:00+00:00  2017-01-18 08:52:00+00:00   \n",
       "1  2017-01-01 00:00:00+00:00  2017-01-18 08:52:00+00:00   \n",
       "2  2017-01-01 00:00:00+00:00  2017-01-18 08:52:00+00:00   \n",
       "3  2017-01-01 00:00:00+00:00  2017-01-18 08:52:00+00:00   \n",
       "4  2017-01-01 00:00:00+00:00  2017-01-18 08:52:00+00:00   \n",
       "\n",
       "  visit_type_concept_name        admitted_from        discharged_to  ...  \\\n",
       "0    EHR encounter record  No matching concept  No matching concept  ...   \n",
       "1    EHR encounter record  No matching concept  No matching concept  ...   \n",
       "2    EHR encounter record  No matching concept  No matching concept  ...   \n",
       "3    EHR encounter record  No matching concept  No matching concept  ...   \n",
       "4    EHR encounter record  No matching concept  No matching concept  ...   \n",
       "\n",
       "  glucose_median_missing  bun_median_missing  creatinine_median_missing  \\\n",
       "0                      1                   1                          1   \n",
       "1                      1                   1                          0   \n",
       "2                      1                   1                          1   \n",
       "3                      1                   1                          1   \n",
       "4                      1                   1                          1   \n",
       "\n",
       "   crp_median_missing  dp_dyn  peep_t  delta_peep  dp_dyn_next  map_next  \\\n",
       "0                   1   32.00    8.00         NaN        32.15      69.0   \n",
       "1                   0   32.15    7.85       -0.15        32.05      71.0   \n",
       "2                   1   32.05    7.95        0.10        32.05      71.0   \n",
       "3                   1   32.05    7.95        0.00        32.00      75.0   \n",
       "4                   1   32.00    8.00        0.05        34.95      78.0   \n",
       "\n",
       "   is_terminal  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/aryanb/aryan personal/code/datathon26/rl/data/data_v1_max_72_h.csv\")  # your dataset\n",
    "df[\"measure_time\"] = pd.to_numeric(df[\"measure_time\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"visit_occurrence_id\", \"measure_time\"]).copy()\n",
    "df = df.sort_values([\"visit_occurrence_id\", \"measure_time\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- carry-forward (LOCF) ----\n",
    "VENT = [\"peep_median\", \"peak_median\"]\n",
    "PHYS = [\"map_median\",\"sbp_median\",\"dbp_median\",\"temp_median\",\n",
    "        \"wbc_median\",\"hemoglobin_median\",\"platelets_median\",\n",
    "        \"sodium_median\",\"potassium_median\",\"chloride_median\",\n",
    "        \"glucose_median\",\"bun_median\",\"creatinine_median\",\"crp_median\"]\n",
    "VENT = [c for c in VENT if c in df.columns]\n",
    "PHYS = [c for c in PHYS if c in df.columns]\n",
    "\n",
    "for c in VENT + PHYS:\n",
    "    df[f\"{c}_missing\"] = df[c].isna().astype(int)\n",
    "\n",
    "# physiology: forward fill within visit\n",
    "df[PHYS] = df.groupby(\"visit_occurrence_id\")[PHYS].ffill()\n",
    "\n",
    "# vent: (option 1) forward fill too, but keep missingness flags so model can learn\n",
    "df[VENT] = df.groupby(\"visit_occurrence_id\")[VENT].ffill()\n",
    "\n",
    "# ---- build dp + transitions ----\n",
    "df[\"dp_dyn\"] = df[\"peak_median\"] - df[\"peep_median\"]\n",
    "\n",
    "# action: PEEP bin or delta-PEEP (we'll do delta bins)\n",
    "df[\"peep_t\"] = df[\"peep_median\"]\n",
    "df[\"delta_peep\"] = df.groupby(\"visit_occurrence_id\")[\"peep_t\"].diff()\n",
    "\n",
    "# next-step labels\n",
    "df[\"dp_dyn_next\"] = df.groupby(\"visit_occurrence_id\")[\"dp_dyn\"].shift(-1)\n",
    "df[\"map_next\"] = df.groupby(\"visit_occurrence_id\")[\"map_median\"].shift(-1) if \"map_median\" in df.columns else np.nan\n",
    "\n",
    "# terminal: last hour in each visit\n",
    "df[\"is_terminal\"] = df.groupby(\"visit_occurrence_id\")[\"measure_time\"].transform(lambda x: x == x.max()).astype(int)\n",
    "\n",
    "# keep usable rows\n",
    "rl = df[df[\"peep_t\"].notna() & df[\"dp_dyn_next\"].notna()].copy()\n",
    "rl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"RL rows:\", len(rl))\n",
    "rl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2efcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_delta(dp):\n",
    "    # clip extreme chart noise\n",
    "    if pd.isna(dp): \n",
    "        return np.nan\n",
    "    dp = float(np.clip(dp, -5, 5))\n",
    "    # bins in cmH2O\n",
    "    if dp <= -2: return -2\n",
    "    if dp == -1: return -1\n",
    "    if dp == 0:  return 0\n",
    "    if dp == 1:  return 1\n",
    "    return 2  # >= +2\n",
    "\n",
    "rl[\"a_bin\"] = rl[\"delta_peep\"].apply(bin_delta)\n",
    "rl = rl.dropna(subset=[\"a_bin\"]).copy()\n",
    "rl[\"a_bin\"] = rl[\"a_bin\"].astype(int)\n",
    "rl[\"reward\"] = -rl[\"dp_dyn_next\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ed4307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00      1020\n",
      "          -1       0.00      0.00      0.00       355\n",
      "           0       0.78      1.00      0.88     35657\n",
      "           1       0.00      0.00      0.00       406\n",
      "           2       0.54      0.01      0.01      8222\n",
      "\n",
      "    accuracy                           0.78     45660\n",
      "   macro avg       0.26      0.20      0.18     45660\n",
      "weighted avg       0.71      0.78      0.69     45660\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FEATURES = [\"peep_t\",\"peak_median\",\"dp_dyn\"] + PHYS\n",
    "FEATURES = [c for c in FEATURES if c in rl.columns]\n",
    "\n",
    "X = rl[FEATURES].fillna(-999)\n",
    "y = rl[\"a_bin\"]\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "pi_b = LogisticRegression(max_iter=2000)\n",
    "pi_b.fit(Xtr, ytr)\n",
    "\n",
    "print(classification_report(yte, pi_b.predict(Xte)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30c599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peep_bin(peep):\n",
    "    if pd.isna(peep): return np.nan\n",
    "    p = float(peep)\n",
    "    if p <= 5: return 5\n",
    "    if p <= 8: return 8\n",
    "    if p <= 10: return 10\n",
    "    if p <= 12: return 12\n",
    "    if p <= 15: return 15\n",
    "    return 20  # 16+\n",
    "\n",
    "rl[\"a_peep_bin\"] = rl[\"peep_t\"].apply(peep_bin).astype(\"Int64\")\n",
    "rl = rl.dropna(subset=[\"a_peep_bin\"]).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7189659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in RL: 233791\n",
      "\n",
      "PEEP-bin distribution:\n",
      "a_peep_bin\n",
      "5     0.551933\n",
      "8     0.332776\n",
      "10    0.068412\n",
      "12    0.019881\n",
      "15    0.013131\n",
      "20    0.013867\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "DP sanity (dp_dyn):\n",
      "count    233449.000000\n",
      "mean         17.986637\n",
      "std          33.119781\n",
      "min       -3244.350000\n",
      "25%          11.000000\n",
      "50%          18.000000\n",
      "75%          25.200000\n",
      "max          66.300000\n",
      "Name: dp_dyn, dtype: float64\n",
      "\n",
      "Using FEATURES: ['peep_t', 'peak_t', 'dp_dyn', 'time_from_admit_h', 'age', 'gender', 'year_of_birth', 'map_median', 'sbp_median', 'dbp_median', 'temp_median', 'wbc_median', 'hemoglobin_median', 'platelets_median', 'sodium_median', 'potassium_median', 'chloride_median', 'glucose_median', 'bun_median', 'creatinine_median', 'crp_median']\n",
      "Numeric cols: ['peep_t', 'peak_t', 'dp_dyn', 'time_from_admit_h', 'age', 'year_of_birth', 'map_median', 'sbp_median', 'dbp_median', 'temp_median', 'wbc_median', 'hemoglobin_median', 'platelets_median', 'sodium_median', 'potassium_median', 'chloride_median', 'glucose_median', 'bun_median', 'creatinine_median', 'crp_median']\n",
      "Categorical cols: ['gender']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/impute/_base.py:641: UserWarning: Skipping features without any observed values: ['bun_median']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Behavior policy report (PEEP-bin) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.93      1.00      0.97     25808\n",
      "           8       0.98      0.85      0.91     15560\n",
      "          10       0.82      0.87      0.84      3199\n",
      "          12       0.75      0.80      0.77       930\n",
      "          15       0.90      0.82      0.86       614\n",
      "          20       1.00      0.94      0.97       648\n",
      "\n",
      "    accuracy                           0.93     46759\n",
      "   macro avg       0.90      0.88      0.89     46759\n",
      "weighted avg       0.94      0.93      0.93     46759\n",
      "\n",
      "Dropping bun_median (all missing).\n",
      "\n",
      "Action bins: [5, 8, 10, 12, 15, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanb/aryan personal/code/datathon26/.venv/lib/python3.13/site-packages/sklearn/impute/_base.py:641: UserWarning: Skipping features without any observed values: ['bun_median']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-model numeric cols: ['peep_t', 'peak_t', 'dp_dyn', 'time_from_admit_h', 'age', 'year_of_birth', 'map_median', 'sbp_median', 'dbp_median', 'temp_median', 'wbc_median', 'hemoglobin_median', 'platelets_median', 'sodium_median', 'potassium_median', 'chloride_median', 'glucose_median', 'creatinine_median', 'crp_median', 'action_bin']\n",
      "Q-model categorical cols: ['gender']\n",
      "\n",
      "=== Q model holdout R^2 (sanity) ===\n",
      "0.9829980012144108\n",
      "\n",
      "Safety training rows (delta_map): 233740\n",
      "\n",
      "=== ΔMAP safety model ===\n",
      "Conformal residual quantile q_delta: 16.118880841545767\n",
      "\n",
      "=== Demo (first 10 rows) ===\n",
      "0 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -10.003528285205153\n",
      "1 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -13.264852218973123\n",
      "2 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -13.934203502427568\n",
      "3 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -13.934203502427568\n",
      "4 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -15.079164201540944\n",
      "5 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -16.32736198093209\n",
      "6 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -15.860067939524809\n",
      "7 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -15.210579641299793\n",
      "8 clin= 8 rec= 8 reason= abstain_hold ΔMAP_lb= -13.507397932184288\n",
      "9 clin= 5 rec= 5 reason= abstain_hold ΔMAP_lb= -12.505839153958275\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 427\u001b[39m\n\u001b[32m    424\u001b[39m Q = np.column_stack([batch_q_for_action(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ACTIONS])\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# If safety is enabled, compute MAP lower bounds for each action too\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m map_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mq\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     MAP_LB = np.column_stack([batch_map_lb_for_action(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ACTIONS])\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Precision Vent MVP (Offline RL on hourly-aggregated dataset)\n",
    "# Dataset assumption:\n",
    "#   - measure_time = hours from admission (can be -1 for pre-admission)\n",
    "#   - rows are hourly bins with *_median/_min/_max/_mean columns\n",
    "#\n",
    "# Goal:\n",
    "#   - Learn a conservative offline policy to choose a PEEP \"level bin\"\n",
    "#   - Objective: minimize next-step dynamic driving pressure dp_dyn_next\n",
    "#   - Safety gate: conformal lower bound on MAP_next >= MAP_MIN\n",
    "#\n",
    "# Output:\n",
    "#   - rl: analysis-ready transition table\n",
    "#   - behavior_policy (pi_b): PEEP-bin classifier\n",
    "#   - q_model: offline Q approximator\n",
    "#   - map_model + conformal quantile q: safety predictor + bound\n",
    "#   - recommend_safe(row): safe recommendation with abstain option\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "CSV_PATH = \"/Users/aryanb/aryan personal/code/datathon26/rl/data/data_v1_max_72_h.csv\"\n",
    "ID_COL   = \"visit_occurrence_id\"\n",
    "T_COL    = \"measure_time\"\n",
    "\n",
    "MAP_MIN = 65         # safety threshold\n",
    "ALPHA   = 0.10       # conformal miscoverage (0.10 -> ~90% bound)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fake datetime (optional; RL only needs ordered time index)\n",
    "MAKE_FAKE_TIMESTAMP = True\n",
    "FAKE_ANCHOR = \"2001-01-01 00:00:00\"\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD + SORT\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[T_COL] = pd.to_numeric(df[T_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[ID_COL, T_COL]).copy()\n",
    "df = df.sort_values([ID_COL, T_COL]).reset_index(drop=True)\n",
    "\n",
    "if MAKE_FAKE_TIMESTAMP:\n",
    "    anchor = pd.Timestamp(FAKE_ANCHOR)\n",
    "    df[\"hour_ts\"] = anchor + pd.to_timedelta(df[T_COL], unit=\"h\")\n",
    "else:\n",
    "    df[\"hour_ts\"] = df[T_COL]\n",
    "\n",
    "# -----------------------------\n",
    "# FEATURE SETS\n",
    "# -----------------------------\n",
    "VENT_COLS = [c for c in [\"peep_median\", \"peak_median\"] if c in df.columns]\n",
    "\n",
    "PHYS_COLS = [\n",
    "    \"map_median\", \"sbp_median\", \"dbp_median\", \"temp_median\",\n",
    "    \"wbc_median\", \"hemoglobin_median\", \"platelets_median\",\n",
    "    \"sodium_median\", \"potassium_median\", \"chloride_median\",\n",
    "    \"glucose_median\", \"bun_median\", \"creatinine_median\", \"crp_median\",\n",
    "]\n",
    "PHYS_COLS = [c for c in PHYS_COLS if c in df.columns]\n",
    "\n",
    "STATIC_COLS = [\"age\", \"gender\", \"year_of_birth\"]\n",
    "STATIC_COLS = [c for c in STATIC_COLS if c in df.columns]\n",
    "\n",
    "if not (\"peep_median\" in df.columns and \"peak_median\" in df.columns):\n",
    "    raise ValueError(\"Need peep_median and peak_median columns for dp_dyn surrogate.\")\n",
    "\n",
    "# -----------------------------\n",
    "# MISSINGNESS FLAGS + CARRY FORWARD (LOCF)\n",
    "# -----------------------------\n",
    "for c in VENT_COLS + PHYS_COLS:\n",
    "    df[f\"{c}_missing\"] = df[c].isna().astype(int)\n",
    "\n",
    "# LOCF within visit\n",
    "if PHYS_COLS:\n",
    "    df[PHYS_COLS] = df.groupby(ID_COL)[PHYS_COLS].ffill()\n",
    "if VENT_COLS:\n",
    "    df[VENT_COLS] = df.groupby(ID_COL)[VENT_COLS].ffill()\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD DP + TRANSITIONS\n",
    "# dp_dyn = peak - peep (surrogate)\n",
    "# -----------------------------\n",
    "df[\"peep_t\"] = df[\"peep_median\"]\n",
    "df[\"peak_t\"] = df[\"peak_median\"]\n",
    "df[\"dp_dyn\"] = df[\"peak_t\"] - df[\"peep_t\"]\n",
    "\n",
    "df[\"dp_dyn_next\"] = df.groupby(ID_COL)[\"dp_dyn\"].shift(-1)\n",
    "df[\"peep_next\"]   = df.groupby(ID_COL)[\"peep_t\"].shift(-1)\n",
    "df[\"peak_next\"]   = df.groupby(ID_COL)[\"peak_t\"].shift(-1)\n",
    "\n",
    "df[\"map_next\"] = df.groupby(ID_COL)[\"map_median\"].shift(-1) if \"map_median\" in df.columns else np.nan\n",
    "\n",
    "df[\"is_terminal\"] = df.groupby(ID_COL)[T_COL].transform(lambda x: (x == x.max())).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# ACTION DEFINITION: PEEP LEVEL BINS\n",
    "# -----------------------------\n",
    "def peep_bin(peep):\n",
    "    if pd.isna(peep):\n",
    "        return np.nan\n",
    "    p = float(peep)\n",
    "    if p <= 5:  return 5\n",
    "    if p <= 8:  return 8\n",
    "    if p <= 10: return 10\n",
    "    if p <= 12: return 12\n",
    "    if p <= 15: return 15\n",
    "    return 20  # 16+\n",
    "\n",
    "df[\"a_peep_bin\"] = df[\"peep_t\"].apply(peep_bin)\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD RL TABLE\n",
    "# -----------------------------\n",
    "rl = df[df[\"a_peep_bin\"].notna() & df[\"dp_dyn_next\"].notna()].copy()\n",
    "rl[\"a_peep_bin\"] = rl[\"a_peep_bin\"].astype(int)\n",
    "\n",
    "rl[\"reward\"] = -rl[\"dp_dyn_next\"]\n",
    "rl[\"time_from_admit_h\"] = rl[T_COL]\n",
    "\n",
    "# -----------------------------\n",
    "# SANITY CHECKS (do not skip)\n",
    "# -----------------------------\n",
    "print(\"Rows in RL:\", len(rl))\n",
    "print(\"\\nPEEP-bin distribution:\")\n",
    "print(rl[\"a_peep_bin\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nDP sanity (dp_dyn):\")\n",
    "print(rl[\"dp_dyn\"].describe())\n",
    "\n",
    "# If dp looks insane, you can clip for MVP:\n",
    "# rl[\"dp_dyn\"] = rl[\"dp_dyn\"].clip(-5, 50)\n",
    "# rl[\"dp_dyn_next\"] = rl[\"dp_dyn_next\"].clip(-5, 50)\n",
    "# rl[\"reward\"] = -rl[\"dp_dyn_next\"]\n",
    "\n",
    "# -----------------------------\n",
    "# DEFINE STATE FEATURES\n",
    "# -----------------------------\n",
    "FEATURES = (\n",
    "    [\"peep_t\", \"peak_t\", \"dp_dyn\", \"time_from_admit_h\"]\n",
    "    + STATIC_COLS\n",
    "    + PHYS_COLS\n",
    ")\n",
    "FEATURES = [c for c in FEATURES if c in rl.columns]\n",
    "print(\"\\nUsing FEATURES:\", FEATURES)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Behavior Policy pi_b(a|s) (PEEP-bin classifier)\n",
    "# Handles categorical columns like gender via OneHotEncoder\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = rl[FEATURES].copy()\n",
    "y = rl[\"a_peep_bin\"].copy()\n",
    "\n",
    "# Identify numeric vs categorical columns\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pi_b = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"lr\", LogisticRegression(\n",
    "        max_iter=8000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"lbfgs\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "pi_b.fit(Xtr, ytr)\n",
    "print(\"\\n=== Behavior policy report (PEEP-bin) ===\")\n",
    "print(classification_report(yte, pi_b.predict(Xte)))\n",
    "\n",
    "# ============================================================\n",
    "# FIX: drop all-missing columns (bun_median) to remove warnings\n",
    "# ============================================================\n",
    "if \"bun_median\" in rl.columns:\n",
    "    if rl[\"bun_median\"].notna().sum() == 0:\n",
    "        print(\"Dropping bun_median (all missing).\")\n",
    "        PHYS_COLS = [c for c in PHYS_COLS if c != \"bun_median\"]\n",
    "        FEATURES = [c for c in FEATURES if c != \"bun_median\"]\n",
    "\n",
    "# ============================================================\n",
    "# Stabilize DP (your min -3244 is a data artifact)\n",
    "# ============================================================\n",
    "rl[\"dp_dyn\"] = rl[\"dp_dyn\"].clip(lower=0, upper=60)\n",
    "rl[\"dp_dyn_next\"] = rl[\"dp_dyn_next\"].clip(lower=0, upper=60)\n",
    "rl[\"reward\"] = -rl[\"dp_dyn_next\"]\n",
    "\n",
    "ACTIONS = sorted(rl[\"a_peep_bin\"].unique().tolist())\n",
    "print(\"\\nAction bins:\", ACTIONS)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Q model with proper (state, action) separation\n",
    "# We will train a model to predict reward from:\n",
    "#   inputs = state FEATURES + candidate action (as categorical)\n",
    "# IMPORTANT: do NOT include a_peep_bin inside FEATURES\n",
    "# ============================================================\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "STATE_FEATURES = FEATURES[:]  # state only (includes gender etc.)\n",
    "if \"a_peep_bin\" in STATE_FEATURES:\n",
    "    STATE_FEATURES.remove(\"a_peep_bin\")\n",
    "\n",
    "# Build training frame: state + observed action as a separate column\n",
    "X_sa = rl[STATE_FEATURES].copy()\n",
    "X_sa[\"action_bin\"] = rl[\"a_peep_bin\"].astype(int)  # separate action column\n",
    "y_q = rl[\"reward\"].copy()\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_sa, y_q, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "num_cols = Xtr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in Xtr.columns if c not in num_cols]\n",
    "\n",
    "print(\"Q-model numeric cols:\", num_cols)\n",
    "print(\"Q-model categorical cols:\", cat_cols)\n",
    "\n",
    "pre_q = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "q_model = Pipeline([\n",
    "    (\"pre\", pre_q),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=50\n",
    "    ))\n",
    "])\n",
    "\n",
    "q_model.fit(Xtr, ytr)\n",
    "print(\"\\n=== Q model holdout R^2 (sanity) ===\")\n",
    "print(q_model.score(Xte, yte))\n",
    "\n",
    "def q_hat(row, a_bin: int) -> float:\n",
    "    x = row[STATE_FEATURES].copy()\n",
    "    x[\"action_bin\"] = int(a_bin)\n",
    "    return float(q_model.predict(pd.DataFrame([x]))[0])\n",
    "\n",
    "def rank_actions_by_q(row):\n",
    "    scores = [(a, q_hat(row, a)) for a in ACTIONS]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5 (REVISED): Safety model on ΔMAP = MAP_next - MAP_current\n",
    "# Conformal lower bound on ΔMAP\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "DELTA_MAP_MIN = -10   # allow up to 10 mmHg drop worst-case (tune: -5, -10, -15)\n",
    "ALPHA = 0.10          # 90% conformal bound\n",
    "\n",
    "# Build label only where both MAP_current and MAP_next exist\n",
    "tmp = rl.copy()\n",
    "tmp = tmp[tmp[\"map_median\"].notna() & tmp[\"map_next\"].notna()].copy()\n",
    "tmp[\"delta_map\"] = tmp[\"map_next\"] - tmp[\"map_median\"]\n",
    "\n",
    "print(\"\\nSafety training rows (delta_map):\", len(tmp))\n",
    "\n",
    "map_model = None\n",
    "q_delta = None\n",
    "\n",
    "if len(tmp) > 5000:  # basic guard\n",
    "    idx = np.arange(len(tmp))\n",
    "    tr_idx, cal_idx = train_test_split(idx, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "    train = tmp.iloc[tr_idx].copy()\n",
    "    calib = tmp.iloc[cal_idx].copy()\n",
    "\n",
    "    X_train = train[STATE_FEATURES].copy()\n",
    "    X_train[\"action_bin\"] = train[\"a_peep_bin\"].astype(int)\n",
    "    y_train = train[\"delta_map\"].copy()\n",
    "\n",
    "    X_cal = calib[STATE_FEATURES].copy()\n",
    "    X_cal[\"action_bin\"] = calib[\"a_peep_bin\"].astype(int)\n",
    "    y_cal = calib[\"delta_map\"].copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "    pre_map = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    map_model = Pipeline([\n",
    "        (\"pre\", pre_map),\n",
    "        (\"gbr\", GradientBoostingRegressor(random_state=RANDOM_SEED))\n",
    "    ])\n",
    "\n",
    "    map_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_cal = map_model.predict(X_cal)\n",
    "    resid = np.abs(y_cal - pred_cal)\n",
    "\n",
    "    q_delta = float(np.quantile(resid, 1 - ALPHA))\n",
    "    print(\"\\n=== ΔMAP safety model ===\")\n",
    "    print(\"Conformal residual quantile q_delta:\", q_delta)\n",
    "else:\n",
    "    print(\"\\nNot enough rows with MAP_current & MAP_next. Safety gate will be OFF.\")\n",
    "\n",
    "def delta_map_pred_lb(row, a_bin: int):\n",
    "    if map_model is None or q_delta is None:\n",
    "        return None, None\n",
    "    x = row[STATE_FEATURES].copy()\n",
    "    x[\"action_bin\"] = int(a_bin)\n",
    "    pred = float(map_model.predict(pd.DataFrame([x]))[0])\n",
    "    lb = pred - q_delta\n",
    "    return pred, lb\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6 (REVISED): Safe recommend using ΔMAP gate\n",
    "# ============================================================\n",
    "def recommend_safe(row):\n",
    "    ranked = rank_actions_by_q(row)\n",
    "\n",
    "    for a, val in ranked:\n",
    "        if map_model is not None:\n",
    "            pred, lb = delta_map_pred_lb(row, a)\n",
    "            if lb < DELTA_MAP_MIN:\n",
    "                continue\n",
    "            return {\"rec\": a, \"q_hat\": val, \"reason\": \"ok\", \"delta_map_pred\": pred, \"delta_map_lb\": lb}\n",
    "\n",
    "        return {\"rec\": a, \"q_hat\": val, \"reason\": \"ok_no_safety\", \"delta_map_pred\": None, \"delta_map_lb\": None}\n",
    "\n",
    "    hold = int(row[\"a_peep_bin\"])\n",
    "    pred, lb = (None, None)\n",
    "    if map_model is not None:\n",
    "        pred, lb = delta_map_pred_lb(row, hold)\n",
    "\n",
    "    return {\"rec\": hold, \"q_hat\": q_hat(row, hold), \"reason\": \"abstain_hold\", \"delta_map_pred\": pred, \"delta_map_lb\": lb}\n",
    "\n",
    "print(\"\\n=== Demo (first 10 rows) ===\")\n",
    "for i in range(min(10, len(rl))):\n",
    "    out = recommend_safe(rl.iloc[i])\n",
    "    print(i, \"clin=\", int(rl.iloc[i][\"a_peep_bin\"]), \"rec=\", out[\"rec\"], \"reason=\", out[\"reason\"], \"ΔMAP_lb=\", out[\"delta_map_lb\"])\n",
    "\n",
    "# ============================================================\n",
    "# FAST STEP 7: vectorized evaluation (no per-row loops)\n",
    "# ============================================================\n",
    "N_EVAL = 20000  # 20k is plenty; change to 5000 if still slow\n",
    "eval_df = rl.sample(min(N_EVAL, len(rl)), random_state=RANDOM_SEED).copy().reset_index(drop=True)\n",
    "\n",
    "# Build base state frame once\n",
    "X_base = eval_df[STATE_FEATURES].copy()\n",
    "\n",
    "def batch_q_for_action(a):\n",
    "    X = X_base.copy()\n",
    "    X[\"action_bin\"] = int(a)\n",
    "    return q_model.predict(X)\n",
    "\n",
    "def batch_map_lb_for_action(a):\n",
    "    if map_model is None or q is None:\n",
    "        return None\n",
    "    X = X_base.copy()\n",
    "    X[\"action_bin\"] = int(a)\n",
    "    mp = map_model.predict(X)\n",
    "    lb = mp - q\n",
    "    return lb\n",
    "\n",
    "# Compute Q for each action in a matrix: shape (n, num_actions)\n",
    "Q = np.column_stack([batch_q_for_action(a) for a in ACTIONS])\n",
    "\n",
    "# If safety is enabled, compute MAP lower bounds for each action too\n",
    "if map_model is not None and q is not None:\n",
    "    MAP_LB = np.column_stack([batch_map_lb_for_action(a) for a in ACTIONS])\n",
    "else:\n",
    "    MAP_LB = None\n",
    "\n",
    "# Choose best action by Q, but only among actions that pass safety\n",
    "if MAP_LB is not None:\n",
    "    safe_mask = (MAP_LB >= MAP_MIN)  # shape (n, num_actions)\n",
    "    # set Q to -inf where unsafe so argmax ignores them\n",
    "    Q_safe = np.where(safe_mask, Q, -np.inf)\n",
    "    best_idx = np.argmax(Q_safe, axis=1)\n",
    "    any_safe = np.any(safe_mask, axis=1)\n",
    "\n",
    "    # If no action safe, abstain -> hold clinician action\n",
    "    hold = eval_df[\"a_peep_bin\"].values.astype(int)\n",
    "    rec = np.where(any_safe, np.array(ACTIONS)[best_idx], hold)\n",
    "    reasons = np.where(any_safe, \"ok\", \"abstain_hold\")\n",
    "else:\n",
    "    best_idx = np.argmax(Q, axis=1)\n",
    "    rec = np.array(ACTIONS)[best_idx]\n",
    "    reasons = np.array([\"ok_no_safety\"] * len(eval_df))\n",
    "\n",
    "clin = eval_df[\"a_peep_bin\"].values.astype(int)\n",
    "\n",
    "# Metrics\n",
    "abstain_rate = float(np.mean(reasons == \"abstain_hold\"))\n",
    "exact_agree = float(np.mean(rec == clin))\n",
    "within1 = float(np.mean(np.abs(\n",
    "    np.vectorize(ACTIONS.index)(rec) - np.vectorize(ACTIONS.index)(clin)\n",
    ") <= 1))\n",
    "\n",
    "# ΔQ(rec - hold)\n",
    "# compute Q for hold and rec in batch\n",
    "hold_idx = np.vectorize(ACTIONS.index)(clin)\n",
    "rec_idx  = np.vectorize(ACTIONS.index)(rec)\n",
    "delta_q = Q[np.arange(len(eval_df)), rec_idx] - Q[np.arange(len(eval_df)), hold_idx]\n",
    "mean_delta_q = float(np.mean(delta_q))\n",
    "\n",
    "print(\"\\n=== FAST Evaluation ===\")\n",
    "print(\"N:\", len(eval_df))\n",
    "print(\"Abstain rate:\", abstain_rate)\n",
    "print(\"Exact agreement:\", exact_agree)\n",
    "print(\"Within-1-bin:\", within1)\n",
    "print(\"Mean predicted ΔQ(rec - hold):\", mean_delta_q)\n",
    "\n",
    "# Optional: if safety enabled, report fraction of rows where ANY action passes\n",
    "if MAP_LB is not None:\n",
    "    print(\"Any-safe fraction:\", float(np.mean(any_safe)))\n",
    "    print(\"Median MAP_LB(best_safe):\", float(np.median(MAP_LB[np.arange(len(eval_df)), best_idx])))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE ARTIFACTS\n",
    "# ============================================================\n",
    "OUT_RL = \"precision_vent_rl_table_mvp.csv\"\n",
    "keep_cols = [ID_COL, \"person_id\", \"hour_ts\", T_COL, \"a_peep_bin\", \"reward\", \"dp_dyn\", \"dp_dyn_next\"] + FEATURES\n",
    "keep_cols = [c for c in keep_cols if c in rl.columns]\n",
    "rl[keep_cols].to_csv(OUT_RL, index=False)\n",
    "print(\"\\nWrote:\", OUT_RL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c83c8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m Xdiag[\u001b[33m\"\u001b[39m\u001b[33maction_bin\u001b[39m\u001b[33m\"\u001b[39m] = diag[\u001b[33m\"\u001b[39m\u001b[33ma_peep_bin\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      8\u001b[39m mp = map_model.predict(Xdiag)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m lb = mp - \u001b[43mq\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMAP_pred median:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(np.median(mp)))\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMAP_LB median:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(np.median(lb)))\n",
      "\u001b[31mNameError\u001b[39m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Safety diagnostic: is the conformal LB always < 65?\n",
    "# ============================================================\n",
    "diag = rl.sample(5000, random_state=RANDOM_SEED).copy().reset_index(drop=True)\n",
    "Xdiag = diag[STATE_FEATURES].copy()\n",
    "Xdiag[\"action_bin\"] = diag[\"a_peep_bin\"].astype(int)\n",
    "\n",
    "mp = map_model.predict(Xdiag)\n",
    "lb = mp - q\n",
    "\n",
    "print(\"MAP_pred median:\", float(np.median(mp)))\n",
    "print(\"MAP_LB median:\", float(np.median(lb)))\n",
    "print(\"Frac LB >= 65:\", float(np.mean(lb >= 65)))\n",
    "print(\"Frac observed MAP_next >= 65:\", float(np.mean(diag['map_next'] >= 65)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7377ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ╔══════════════════════════════════════════════════════════════════════╗\n",
      "    ║     PRECISION VENTILATION RL - IMPROVED REWARD IMPLEMENTATION        ║\n",
      "    ╚══════════════════════════════════════════════════════════════════════╝\n",
      "    \n",
      "\n",
      "[STEP 1/5] Checking data requirements...\n",
      "======================================================================\n",
      "DATA REQUIREMENTS CHECK\n",
      "======================================================================\n",
      "\n",
      "REQUIRED: ✗ MISSING DATA\n",
      "  Available: visit_occurrence_id, measure_time\n",
      "  Missing:   is_terminal\n",
      "\n",
      "OPTION1: ✓ READY\n",
      "  Available: discharged_to (as outcome)\n",
      "\n",
      "OPTION2: ✗ MISSING DATA\n",
      "  Available: discharged_to (as outcome)\n",
      "  Missing:   spo2\n",
      "\n",
      "OPTION3: ✗ MISSING DATA\n",
      "  Available: discharged_to (as outcome)\n",
      "  Missing:   spo2, fio2, peep_t\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATION:\n",
      "⚠ Only Option 1 available - need to add SpO2 for better results\n",
      "======================================================================\n",
      "\n",
      "[STEP 2/5] Preparing data...\n",
      "Creating 'outcome' from 'discharged_to'...\n",
      "Created 'outcome' column:\n",
      "outcome\n",
      "Unknown    3270431\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[STEP 3/5] No old rewards to compare (or compare_old=False)\n",
      "\n",
      "[STEP 4/5] Computing new rewards (Option 2)...\n",
      "Using: Hybrid Oxygenation Reward (RECOMMENDED)\n",
      "\n",
      "[STEP 5/5] Validating new rewards...\n",
      "\n",
      "NEW REWARDS VALIDATION\n",
      "======================================================================\n",
      "Total rows:    3,270,431\n",
      "Non-zero:      0 (0.0%)\n",
      "Sparsity:      100.0%\n",
      "Mean:          0.0000\n",
      "Std:           0.0000\n",
      "Range:         [0.00, 0.00]\n",
      "\n",
      "Distribution:\n",
      "  Positive:    0 (0.0%)\n",
      "  Negative:    0 (0.0%)\n",
      "  Zero:        3,270,431 (100.0%)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "✓ COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Review the validation statistics above\n",
      "2. If satisfied, replace old rewards:\n",
      "   >>> df['reward'] = df['reward_new']\n",
      "3. Continue with your Q-learning training using new rewards\n",
      "4. Compare policies (optional)\n",
      "\n",
      "To use new rewards immediately:\n",
      "   >>> df['reward'] = df['reward_new']\n",
      "   >>> df = df.drop('reward_new', axis=1)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from ventilation_rewards import friday_implementation\n",
    "\n",
    "rl = pd.read_csv(\"/Users/aryanb/aryan personal/code/datathon26/rl/data/data_v3_max_72_h.csv\")\n",
    "rl = friday_implementation(rl, option=2)  # Option 2 = Hybrid Oxygenation (recommended)\n",
    "rl['reward'] = rl['reward_new']  # Replace old rewards\n",
    "\n",
    "# Now use rl['reward'] in your Q-learning training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f2491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Loading data\n",
      "======================================================================\n",
      "✓ Data loaded: 3,270,431 rows, 95 columns\n",
      "\n",
      "First few rows:\n",
      "   visit_occurrence_id  measure_time  person_id gender  year_of_birth  \\\n",
      "0                    0            -1      54818   MALE           1935   \n",
      "1                    0             0      54818   MALE           1935   \n",
      "2                    0             1      54818   MALE           1935   \n",
      "\n",
      "        visit_start_datetime         visit_end_datetime  \\\n",
      "0  2010-01-01 00:00:00+00:00  2010-01-01 17:39:00+00:00   \n",
      "1  2010-01-01 00:00:00+00:00  2010-01-01 17:39:00+00:00   \n",
      "2  2010-01-01 00:00:00+00:00  2010-01-01 17:39:00+00:00   \n",
      "\n",
      "  visit_type_concept_name        admitted_from        discharged_to  ...  \\\n",
      "0    EHR encounter record  No matching concept  No matching concept  ...   \n",
      "1    EHR encounter record  No matching concept  No matching concept  ...   \n",
      "2    EHR encounter record  No matching concept  No matching concept  ...   \n",
      "\n",
      "  chloride_max  chloride_mean  creatinine_median  creatinine_min  \\\n",
      "0          NaN            NaN                NaN             NaN   \n",
      "1          NaN            NaN          93.000003       93.000003   \n",
      "2          NaN            NaN                NaN             NaN   \n",
      "\n",
      "  creatinine_max  creatinine_mean  crp_median  crp_min  crp_max  crp_mean  \n",
      "0            NaN              NaN         NaN      NaN      NaN       NaN  \n",
      "1      93.000003        93.000003         NaN      NaN      NaN       NaN  \n",
      "2            NaN              NaN         NaN      NaN      NaN       NaN  \n",
      "\n",
      "[3 rows x 95 columns]\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Checking available columns\n",
      "======================================================================\n",
      "Total columns: 95\n",
      "\n",
      "Column names:\n",
      "  1. visit_occurrence_id\n",
      "  2. measure_time\n",
      "  3. person_id\n",
      "  4. gender\n",
      "  5. year_of_birth\n",
      "  6. visit_start_datetime\n",
      "  7. visit_end_datetime\n",
      "  8. visit_type_concept_name\n",
      "  9. admitted_from\n",
      "  10. discharged_to\n",
      "  11. death_datetime\n",
      "  12. length_of_stay_hours\n",
      "  13. death_hours\n",
      "  14. age\n",
      "  15. vent_mode\n",
      "  16. fio2_median\n",
      "  17. fio2_min\n",
      "  18. fio2_max\n",
      "  19. fio2_mean\n",
      "  20. map_median\n",
      "  21. map_min\n",
      "  22. map_max\n",
      "  23. map_mean\n",
      "  24. peep_median\n",
      "  25. peep_min\n",
      "  26. peep_max\n",
      "  27. peep_mean\n",
      "  28. sbp_median\n",
      "  29. sbp_min\n",
      "  30. sbp_max\n",
      "  31. sbp_mean\n",
      "  32. dbp_median\n",
      "  33. dbp_min\n",
      "  34. dbp_max\n",
      "  35. dbp_mean\n",
      "  36. spo2_median\n",
      "  37. spo2_min\n",
      "  38. spo2_max\n",
      "  39. spo2_mean\n",
      "  40. peak_median\n",
      "  41. peak_min\n",
      "  42. peak_max\n",
      "  43. peak_mean\n",
      "  44. temp_median\n",
      "  45. temp_min\n",
      "  46. temp_max\n",
      "  47. temp_mean\n",
      "  48. potassium_median\n",
      "  49. potassium_min\n",
      "  50. potassium_max\n",
      "  51. potassium_mean\n",
      "  52. ph_median\n",
      "  53. ph_min\n",
      "  54. ph_max\n",
      "  55. ph_mean\n",
      "  56. glucose_median\n",
      "  57. glucose_min\n",
      "  58. glucose_max\n",
      "  59. glucose_mean\n",
      "  60. hemoglobin_median\n",
      "  61. hemoglobin_min\n",
      "  62. hemoglobin_max\n",
      "  63. hemoglobin_mean\n",
      "  64. pao2_median\n",
      "  65. pao2_min\n",
      "  66. pao2_max\n",
      "  67. pao2_mean\n",
      "  68. sao2_median\n",
      "  69. sao2_min\n",
      "  70. sao2_max\n",
      "  71. sao2_mean\n",
      "  72. wbc_median\n",
      "  73. wbc_min\n",
      "  74. wbc_max\n",
      "  75. wbc_mean\n",
      "  76. platelets_median\n",
      "  77. platelets_min\n",
      "  78. platelets_max\n",
      "  79. platelets_mean\n",
      "  80. sodium_median\n",
      "  81. sodium_min\n",
      "  82. sodium_max\n",
      "  83. sodium_mean\n",
      "  84. chloride_median\n",
      "  85. chloride_min\n",
      "  86. chloride_max\n",
      "  87. chloride_mean\n",
      "  88. creatinine_median\n",
      "  89. creatinine_min\n",
      "  90. creatinine_max\n",
      "  91. creatinine_mean\n",
      "  92. crp_median\n",
      "  93. crp_min\n",
      "  94. crp_max\n",
      "  95. crp_mean\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Checking required columns for rewards\n",
      "======================================================================\n",
      "\n",
      "Basic (always needed):\n",
      "  ✓ visit_occurrence_id: 3,270,431 values (100.0% coverage)\n",
      "  ✓ measure_time: 3,270,431 values (100.0% coverage)\n",
      "  ✗ is_terminal: NOT FOUND\n",
      "\n",
      "For Option 2 (Hybrid):\n",
      "  ✗ spo2: NOT FOUND\n",
      "  ✗ outcome: NOT FOUND\n",
      "\n",
      "Alternative outcome:\n",
      "  ✓ discharged_to: 3,270,431 values (100.0% coverage)\n",
      "\n",
      "======================================================================\n",
      "STEP 4: SpO2 availability (critical for Option 2)\n",
      "======================================================================\n",
      "✗ SpO2 column NOT FOUND\n",
      "\n",
      "Possible SpO2 column names in your data:\n",
      "  - fio2_median\n",
      "  - fio2_min\n",
      "  - fio2_max\n",
      "  - fio2_mean\n",
      "  - spo2_median\n",
      "  - spo2_min\n",
      "  - spo2_max\n",
      "  - spo2_mean\n",
      "  - pao2_median\n",
      "  - pao2_min\n",
      "  - pao2_max\n",
      "  - pao2_mean\n",
      "  - sao2_median\n",
      "  - sao2_min\n",
      "  - sao2_max\n",
      "  - sao2_mean\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Outcome/mortality data\n",
      "======================================================================\n",
      "✓ 'discharged_to' column exists (will be converted to 'outcome')\n",
      "discharged_to\n",
      "No matching concept    3270431\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Terminal states\n",
      "======================================================================\n",
      "✗ 'is_terminal' column NOT FOUND\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "✗ MISSING REQUIRED DATA\n",
      "\n",
      "Missing:\n",
      "  - Basic columns: ['is_terminal']\n",
      "  - SpO2 column\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Copy the entire contents of debug_data.py into a cell\n",
    "# Or if the file is accessible, just run:\n",
    "%run debug_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914b969",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c50c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "✓ Loaded: 3,270,431 rows\n",
      "\n",
      "Adding is_terminal column...\n",
      "✓ Added is_terminal: 74,186 terminal states\n",
      "\n",
      "Creating outcome column...\n",
      "Outcome distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Checking for SpO2...\n",
      "Found SpO2 columns: ['spo2_median', 'spo2_min', 'spo2_max', 'spo2_mean']\n",
      "✓ Using 'spo2_median' as spo2: 939,292 values\n",
      "\n",
      "======================================================================\n",
      "COMPUTING REWARDS\n",
      "======================================================================\n",
      "\n",
      "Using Option 2: Hybrid Oxygenation (with SpO2)\n",
      "Applying carry-forward imputation...\n",
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "\n",
      "Reward statistics:\n",
      "  Total rows:     3,270,431\n",
      "  Non-zero:       937,754 (28.7%)\n",
      "  Mean:           0.0797\n",
      "  Std:            0.3165\n",
      "  Range:          [-106.27, 106.30]\n",
      "\n",
      "Distribution:\n",
      "  Positive:       751,373\n",
      "  Negative:       186,381\n",
      "  Zero:           2,332,677\n",
      "\n",
      "======================================================================\n",
      "✓ COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Your 'rl' dataframe now has a 'reward' column.\n",
      "You can use it in your Q-learning training!\n",
      "\n",
      "Next: Continue with your Q-learning code using rl['reward']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════\n",
    "SINGLE CELL SOLUTION - COPY THIS ENTIRE CELL AND RUN IT\n",
    "═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "This ONE cell does EVERYTHING:\n",
    "1. Loads your data\n",
    "2. Adds missing columns (is_terminal, outcome)  \n",
    "3. Finds SpO2 data if it exists\n",
    "4. Computes new rewards\n",
    "5. Gives you a ready-to-use dataframe\n",
    "\n",
    "Just update the file path and run!\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 0: UPDATE THIS PATH TO YOUR FILE\n",
    "# ============================================================================\n",
    "FILE_PATH = \"/Users/aryanb/aryan personal/code/datathon26/rl/data/data_v3_max_72_h.csv\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "rl = pd.read_csv(FILE_PATH)\n",
    "print(f\"✓ Loaded: {len(rl):,} rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Add is_terminal column (marks last timestep for each visit)\n",
    "# ============================================================================\n",
    "print(\"\\nAdding is_terminal column...\")\n",
    "rl = rl.sort_values(['visit_occurrence_id', 'measure_time'])\n",
    "rl['is_terminal'] = 0\n",
    "last_rows = rl.groupby('visit_occurrence_id').tail(1).index\n",
    "rl.loc[last_rows, 'is_terminal'] = 1\n",
    "print(f\"✓ Added is_terminal: {(rl['is_terminal']==1).sum():,} terminal states\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Create outcome column from discharged_to\n",
    "# ============================================================================\n",
    "print(\"\\nCreating outcome column...\")\n",
    "\n",
    "def map_outcome(val):\n",
    "    if pd.isna(val) or val == 'No matching concept':\n",
    "        return None  # Use None for non-terminal rows\n",
    "    val_lower = str(val).lower()\n",
    "    \n",
    "    if any(term in val_lower for term in ['home', 'alive', 'survived', 'discharge', 'rehab']):\n",
    "        return 'Survived'\n",
    "    elif any(term in val_lower for term in ['died', 'death', 'deceased', 'expired']):\n",
    "        return 'Died'\n",
    "    return 'Unknown'\n",
    "\n",
    "rl['outcome'] = rl['discharged_to'].apply(map_outcome)\n",
    "# Only keep outcome for terminal rows\n",
    "rl.loc[rl['is_terminal'] == 0, 'outcome'] = np.nan\n",
    "\n",
    "print(\"Outcome distribution:\")\n",
    "print(rl[rl['is_terminal']==1]['outcome'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Check for SpO2 data\n",
    "# ============================================================================\n",
    "print(\"\\nChecking for SpO2...\")\n",
    "\n",
    "# Look for SpO2 in any column name\n",
    "spo2_cols = [col for col in rl.columns if 'spo2' in col.lower()]\n",
    "\n",
    "if spo2_cols:\n",
    "    print(f\"Found SpO2 columns: {spo2_cols}\")\n",
    "    # Use median if available, otherwise first match\n",
    "    if any('median' in col.lower() for col in spo2_cols):\n",
    "        source = [col for col in spo2_cols if 'median' in col.lower()][0]\n",
    "    else:\n",
    "        source = spo2_cols[0]\n",
    "    \n",
    "    rl['spo2'] = rl[source]\n",
    "    print(f\"✓ Using '{source}' as spo2: {rl['spo2'].notna().sum():,} values\")\n",
    "    HAS_SPO2 = True\n",
    "else:\n",
    "    print(\"✗ No SpO2 data found\")\n",
    "    HAS_SPO2 = False\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Compute rewards\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING REWARDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if HAS_SPO2:\n",
    "    print(\"\\nUsing Option 2: Hybrid Oxygenation (with SpO2)\")\n",
    "    \n",
    "    # Carry-forward imputation for SpO2\n",
    "    print(\"Applying carry-forward imputation...\")\n",
    "    rl['spo2'] = rl.groupby('visit_occurrence_id')['spo2'].ffill()\n",
    "    \n",
    "    # Compute hybrid rewards\n",
    "    rewards = np.zeros(len(rl))\n",
    "    \n",
    "    for idx in range(len(rl)):\n",
    "        row = rl.iloc[idx]\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Intermediate rewards from SpO2\n",
    "        if idx + 1 < len(rl):\n",
    "            next_row = rl.iloc[idx + 1]\n",
    "            if row['visit_occurrence_id'] == next_row['visit_occurrence_id']:\n",
    "                # Delta SpO2\n",
    "                if pd.notna(row['spo2']) and pd.notna(next_row['spo2']):\n",
    "                    delta_spo2 = next_row['spo2'] - row['spo2']\n",
    "                    reward += 0.1 * delta_spo2\n",
    "                \n",
    "                # Target range bonus (92-96%)\n",
    "                if pd.notna(next_row['spo2']):\n",
    "                    if 92 <= next_row['spo2'] <= 96:\n",
    "                        reward += 0.5\n",
    "        \n",
    "        # Terminal reward\n",
    "        if row['is_terminal'] == 1:\n",
    "            if row['outcome'] == 'Survived':\n",
    "                reward += 100.0\n",
    "            elif row['outcome'] == 'Died':\n",
    "                reward -= 100.0\n",
    "        \n",
    "        rewards[idx] = reward\n",
    "    \n",
    "    rl['reward'] = rewards\n",
    "    \n",
    "else:\n",
    "    print(\"\\nUsing Option 1: Pure Terminal (no SpO2)\")\n",
    "    \n",
    "    # Pure terminal rewards\n",
    "    rewards = np.zeros(len(rl))\n",
    "    \n",
    "    for idx in range(len(rl)):\n",
    "        row = rl.iloc[idx]\n",
    "        if row['is_terminal'] == 1:\n",
    "            if row['outcome'] == 'Survived':\n",
    "                rewards[idx] = 100.0\n",
    "            elif row['outcome'] == 'Died':\n",
    "                rewards[idx] = -100.0\n",
    "    \n",
    "    rl['reward'] = rewards\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Validate results\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nReward statistics:\")\n",
    "print(f\"  Total rows:     {len(rl):,}\")\n",
    "print(f\"  Non-zero:       {(rl['reward'] != 0).sum():,} ({(rl['reward'] != 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Mean:           {rl['reward'].mean():.4f}\")\n",
    "print(f\"  Std:            {rl['reward'].std():.4f}\")\n",
    "print(f\"  Range:          [{rl['reward'].min():.2f}, {rl['reward'].max():.2f}]\")\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  Positive:       {(rl['reward'] > 0).sum():,}\")\n",
    "print(f\"  Negative:       {(rl['reward'] < 0).sum():,}\")\n",
    "print(f\"  Zero:           {(rl['reward'] == 0).sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYour 'rl' dataframe now has a 'reward' column.\")\n",
    "print(\"You can use it in your Q-learning training!\")\n",
    "\n",
    "if not HAS_SPO2:\n",
    "    print(\"\\n⚠ NOTE: Rewards are sparse because no SpO2 data was found.\")\n",
    "    print(\"Consider adding SpO2 data for better results (Option 2).\")\n",
    "\n",
    "print(\"\\nNext: Continue with your Q-learning code using rl['reward']\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
